{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>   Cookiecutter-Pytorch-Lightning  </p> <p>This modern Cookiecutter template provides all the necessary tools for deep learning development, training, testing, and deployment. It supports the following features:</p> <p>Deep Learning related</p> <ul> <li>PyTorch and PyTorch Lightning for deep learning framework</li> <li>Hydra for modular experiment configuration</li> <li>Optional experiment trackers: Tensorboard, W&amp;B, Neptune, Comet, MLFlow and CSVLogger</li> </ul> <p>Python related</p> <ul> <li>uv for dependency management</li> <li>Supports both src and flat layout.</li> <li>CI/CD with GitHub Actions</li> <li>Pre-commit hooks with pre-commit</li> <li>Code quality with ruff, mypy and deptry.</li> <li>Publishing to PyPI by creating a new release on GitHub</li> <li>Testing and coverage with pytest and codecov</li> <li>Documentation with MkDocs</li> <li>Compatibility testing for multiple versions of Python with tox-uv</li> <li>Containerization with Docker or Podman</li> <li>Development environment with VSCode devcontainers</li> </ul> <p>An example of a repository generated with this package can be found here.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>On your local machine, navigate to the directory in which you want to create a project directory, and run the following command:</p> <pre><code>uvx cookiecutter https://github.com/foreverYoungGitHub/cookiecutter-pytorch-lightning.git\n</code></pre> <p>or if you don't have <code>uv</code> installed yet:</p> <pre><code>pip install cookiecutter\ncookiecutter https://github.com/foreverYoungGitHub/cookiecutter-pytorch-lightning.git\n</code></pre> <p>Follow the prompts to configure your project. Once completed, a new directory containing your project will be created. Then navigate into your newly created project directory and follow the instructions in the <code>README.md</code> to complete the setup of your project.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project is partially based on Florian Maas\\'s great cookiecutter-uv and Lukas\\'s great lightning-hydra-template repository.</p>"},{"location":"prompt_arguments/","title":"Prompt arguments","text":"<p>When running the command <code>ccp</code> a prompt will start which enables you to configure your repository. The prompt values and their explanation are as follows:</p> <p>author</p> <p>Your full name.</p> <p>email</p> <p>Your email address.</p> <p>author_github_handle</p> <p>Your github handle, i.e. <code>&lt;handle&gt;</code> in <code>https://github.com/&lt;handle&gt;</code></p> <p>project_name</p> <p>Your project name. Should be equal to the name of your repository and it should only contain alphanumeric characters and <code>-</code>'s.</p> <p>project_slug</p> <p>The project slug, will default to the <code>project_name</code> with all <code>-</code>'s replaced with <code>_</code>. This will be how you import your code later, e.g.</p> <pre><code>from &lt;project_slug&gt; import foo\n</code></pre> <p>project_description</p> <p>A short description of your project.</p> <p>layout</p> <p><code>\"flat\"</code> or <code>\"src\"</code>, defaults to <code>\"flat\"</code>.</p> <ul> <li><code>\"flat\"</code>: Places the Python module in the root directory.</li> <li><code>\"src\"</code>: Organizes the project by placing the Python module inside a <code>src</code> directory.</li> </ul> <p>include_github_actions</p> <p><code>\"y\"</code> or <code>\"n\"</code>. Adds a <code>.github</code> directory with various actions and workflows to setup the environment and run code formatting checks and unittests.</p> <p>publish_to_pypi</p> <p><code>\"y\"</code> or <code>\"n\"</code>. Adds functionality to the <code>Makefile</code> and Github workflows to make publishing your code as simple as creating a new release release on Github. For more info, see Publishing to PyPI.</p> <p>mypy</p> <p><code>\"n\"</code> or <code>\"y\"</code>. Adds mypy static type checking to the development dependencies and includes it in the <code>make check</code> command. Note: It is strongly suggested to select \"n\" (no) for the pytorch lightning project, as mypy can be challenging to configure properly with PyTorch Lightning projects and may cause unnecessary complexity during development.</p> <p>deptry</p> <p><code>\"y\"</code> or <code>\"n\"</code>. Adds deptry to the development dependencies of the project, and adds it to the <code>make check</code> command. <code>deptry</code> is a command line tool to check for issues with dependencies in a Python project, such as obsolete or missing dependencies.</p> <p>mkdocs</p> <p><code>\"y\"</code> or <code>\"n\"</code>. Adds MkDocs documentation to your project. This includes automatically parsing your docstrings and adding them to the documentation. Documentation will be deployed to the <code>gh-pages</code> branch.</p> <p>codecov</p> <p><code>\"y\"</code> or <code>\"n\"</code>. Adds code coverage checks with codecov.</p> <p>dockerfile</p> <p><code>\"y\"</code> or <code>\"n\"</code>. Adds a simple Dockerfile.</p> <p>devcontainer</p> <p><code>\"y\"</code> or <code>\"n\"</code>. Adds a devcontainer specification to the project along with pre-installed pre-commit hooks and VSCode python extension configuration.</p> <p>open_source_license</p> <p>Choose a license. Options: <code>[\"1. MIT License\", \"2. BSD license\", \"3. ISC license\",  \"4. Apache Software License 2.0\", \"5. GNU General Public License v3\", \"6. Not open source\"]</code></p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This page contains a complete tutorial on how to create your project.</p>"},{"location":"tutorial/#step-1-install-uv","title":"Step 1: Install uv","text":"<p>To start, we will need to install <code>uv</code>. The instructions to install uv can be found here. For MacOS or Linux;</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"tutorial/#step-2-generate-your-project","title":"Step 2: Generate your project","text":"<p>On your local machine, navigate to the directory in which you want to create a project directory, and run the following command:</p> <pre><code>uvx cookiecutter https://github.com/foreverYoungGitHub/cookiecutter-pytorch-lightning.git\n</code></pre> <p>For an explanation of the prompt arguments, see Prompt Arguments.</p>"},{"location":"tutorial/#step-3-set-up-your-github-repository","title":"Step 3: Set up your Github repository","text":"<p>Create an empty new repository on Github. Give it a name that only contains alphanumeric characters and optionally <code>-</code>. DO NOT check any boxes under the option <code>Initialize this repository with</code>.</p>"},{"location":"tutorial/#step-4-upload-your-project-to-github","title":"Step 4: Upload your project to Github","text":"<p>Run the following commands, replacing <code>&lt;project-name&gt;</code> with the name that you also gave the Github repository and <code>&lt;github_author_handle&gt;</code> with your Github username.</p> <pre><code>cd &lt;project_name&gt;\ngit init -b main\ngit add .\ngit commit -m \"Init commit\"\ngit remote add origin git@github.com:&lt;github_author_handle&gt;/&lt;project_name&gt;.git\ngit push -u origin main\n</code></pre>"},{"location":"tutorial/#step-5-set-up-your-development-environment","title":"Step 5: Set Up Your Development Environment","text":"<p>Initially, the CI/CD pipeline will fail for two reasons:</p> <ul> <li>The project does not yet contain a <code>uv.lock</code> file</li> <li>There are a few formatting issues in the project</li> </ul> <p>To fix that, we first install the environment and the pre-commit hooks with:</p> <pre><code>make install\n</code></pre> <p>This will generate the <code>uv.lock</code> file</p>"},{"location":"tutorial/#step-6-run-the-pre-commit-hooks","title":"Step 6: Run the pre-commit hooks","text":"<p>Now, to resolve the formatting issues, let's run the pre-commit hooks:</p> <pre><code>uv run pre-commit run -a\n</code></pre>"},{"location":"tutorial/#7-commit-the-changes","title":"7. Commit the changes","text":"<p>Now we commit the changes made by the two steps above to the repository:</p> <pre><code>git add .\ngit commit -m 'Fix formatting issues'\ngit push origin main\n</code></pre>"},{"location":"tutorial/#step-8-sign-up-to-codecovio","title":"Step 8: Sign up to codecov.io","text":"<p>If you enabled code coverage with codecov for your project, you should sign up with your GitHub account at codecov.io</p>"},{"location":"tutorial/#step-9-configure-your-repository-secrets","title":"Step 9: Configure your repository secrets","text":"<p>If you want to deploy your project to PyPI using the Github Actions, you will have to set some repository secrets. For instructions on how to do that, see here.</p>"},{"location":"tutorial/#step-10-enable-your-documentation","title":"Step 10: Enable your documentation","text":"<p>To enable your documentation on GitHub, first navigate to <code>Settings &gt; Actions &gt; General</code> in your repository, and under <code>Workflow permissions</code> select <code>Read and write permissions</code>.</p>"},{"location":"tutorial/#step-11-create-a-new-release","title":"Step 11: Create a new release","text":"<p>To trigger a new release, navigate to your repository on GitHub, click <code>Releases</code> on the right, and then select <code>Draft a new release</code>. If you fail to find the button, you could also directly visit <code>https://github.com/&lt;username&gt;/&lt;repository-name&gt;/releases/new</code>.</p> <p>Give your release a title, and add a new tag in the form <code>*.*.*</code> where the <code>*</code>'s are alphanumeric. To finish, press <code>Publish release</code>.</p>"},{"location":"tutorial/#step-12-enable-your-documentation-ctd","title":"Step 12: Enable your documentation ct'd","text":"<p>Then navigate to <code>Settings &gt; Code and Automation &gt; Pages</code>. If you succesfully created a new release, you should see a notification saying <code>Your site is ready to be published at https://&lt;author_github_handle&gt;.github.io/&lt;project_name&gt;/</code>.</p> <p>To finalize deploying your documentation, under <code>Source</code>, select the branch <code>gh-pages</code>.</p>"},{"location":"tutorial/#step-12-youre-all-set","title":"Step 12: You're all set!","text":"<p>That's it! I hope this repository saved you a lot of manual configuration. If you have any improvement suggestions, feel free to raise an issue or open a PR on Github!</p>"},{"location":"dl_features/environment/","title":"Setup Environment","text":"<p>This project uses uv for dependency management and provides a streamlined setup process for deep learning development with PyTorch Lightning.</p>"},{"location":"dl_features/environment/#installation","title":"Installation","text":""},{"location":"dl_features/environment/#option-1-using-make-recommended","title":"Option 1: Using Make (Recommended)","text":"<p>The project includes a convenient Makefile for setup:</p> <pre><code>make install\n</code></pre> <p>This command will: - Create a virtual environment using uv - Install all dependencies from <code>pyproject.toml</code> - Set up pre-commit hooks for code quality</p>"},{"location":"dl_features/environment/#verification","title":"Verification","text":"<p>To verify your installation is working correctly:</p> <pre><code># Run code quality checks\nmake check\n\n# Run tests\nmake test\n\n# Check if PyTorch can detect your GPU (if available)\nuv run python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n\n# Try your first train with\nuv run python -m {{cookiecutter.project_slug}}.scripts.train\n</code></pre> <p>Since it create the virtual environment using uv, please use uv run for all your python script like <code>uv run python</code> or just <code>source ./.venv/bin/activate</code> to enter the environment first and then run python command.</p>"},{"location":"dl_features/environment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"dl_features/environment/#common-issues","title":"Common Issues","text":"<p>uv not found: Install uv using <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code> or visit uv installation guide</p> <p>CUDA version mismatch: This should be handled by UV properly. But If you want to specific version, please check using uv with PyTorch</p> <p>Pre-commit hooks failing: Run <code>uv run pre-commit install</code> and <code>uv run pre-commit run --all-files</code> to set up and test hooks.</p> <p>For additional help, see the project's GitHub repository issues section.</p>"},{"location":"dl_features/evaluation/","title":"Evaluation","text":"<p>Evaluate trained models on test datasets using PyTorch Lightning's evaluation capabilities.</p>"},{"location":"dl_features/evaluation/#basic-evaluation","title":"Basic Evaluation","text":""},{"location":"dl_features/evaluation/#evaluate-latest-checkpoint","title":"Evaluate Latest Checkpoint","text":"<p>Evaluate the most recent checkpoint:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.eval\n</code></pre> <p>This automatically finds and loads the best checkpoint from your latest training run.</p>"},{"location":"dl_features/evaluation/#evaluate-specific-checkpoint","title":"Evaluate Specific Checkpoint","text":"<p>Evaluate a specific checkpoint file:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.eval ckpt_path=\"/path/to/checkpoint.ckpt\"\n</code></pre>"},{"location":"dl_features/evaluation/#evaluate-with-custom-data","title":"Evaluate with Custom Data","text":"<p>Override the default test dataset:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.eval data.test_path=\"/path/to/test/data\"\n</code></pre>"},{"location":"dl_features/evaluation/#hardware-configuration","title":"Hardware Configuration","text":""},{"location":"dl_features/evaluation/#cpu-evaluation","title":"CPU Evaluation","text":"<p>Force CPU evaluation:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.eval trainer.accelerator=cpu\n</code></pre>"},{"location":"dl_features/evaluation/#gpu-evaluation","title":"GPU Evaluation","text":"<p>Use GPU for faster evaluation:</p> <pre><code># Single GPU (automatic if available)\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval\n\n# Specify GPU device\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval trainer.devices=1 trainer.accelerator=gpu\n</code></pre>"},{"location":"dl_features/evaluation/#multi-gpu-evaluation","title":"Multi-GPU Evaluation","text":"<p>Distribute evaluation across multiple GPUs:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.eval trainer.devices=4 trainer.strategy=ddp\n</code></pre>"},{"location":"dl_features/evaluation/#evaluation-outputs","title":"Evaluation Outputs","text":""},{"location":"dl_features/evaluation/#results-logging","title":"Results Logging","text":"<p>Evaluation results are automatically saved to:</p> <ul> <li>Console: Immediate results display</li> <li>CSV Files: Detailed metrics in <code>logs/eval/</code> (when CSV logger is configured)</li> <li>TensorBoard: Visual metrics in tensorboard logs (when TensorBoard logger is configured)</li> <li>Weights &amp; Biases: Remote experiment tracking (when wandb logger is configured)</li> <li>Comet: ML experiment tracking and monitoring (when Comet logger is configured)</li> <li>Neptune: ML experiment management (when Neptune logger is configured)</li> <li>MLflow: Experiment tracking dashboard (when MLflow logger is configured)</li> </ul>"},{"location":"dl_features/evaluation/#batch-size-optimization","title":"Batch Size Optimization","text":"<p>Optimize batch size for evaluation speed:</p> <pre><code># Larger batch for faster evaluation\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval data.batch_size=128\n\n# Smaller batch if memory constrained\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval data.batch_size=32\n</code></pre>"},{"location":"dl_features/evaluation/#debugging-evaluation","title":"Debugging Evaluation","text":""},{"location":"dl_features/evaluation/#verbose-output","title":"Verbose Output","text":"<p>Enable detailed logging during evaluation:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.eval trainer.logger.level=DEBUG\n</code></pre>"},{"location":"dl_features/evaluation/#limit-evaluation-batches","title":"Limit Evaluation Batches","text":"<p>Quick evaluation for debugging:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.eval debug=limit\n</code></pre>"},{"location":"dl_features/evaluation/#profile-evaluation","title":"Profile Evaluation","text":"<p>Profile evaluation performance:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.eval debug=profiler\n</code></pre>"},{"location":"dl_features/evaluation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"dl_features/evaluation/#memory-issues","title":"Memory Issues","text":"<p>If evaluation runs out of memory:</p> <pre><code># Reduce batch size\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval data.batch_size=16\n\n# Use CPU evaluation\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval trainer.accelerator=cpu\n\n# Use lower precision\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval ++trainer.precision=16-mixed\n</code></pre>"},{"location":"dl_features/evaluation/#checkpoint-loading-issues","title":"Checkpoint Loading Issues","text":"<p>If checkpoint fails to load:</p> <pre><code># Check checkpoint path\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval ckpt_path=\"/absolute/path/to/checkpoint.ckpt\"\n\n# Check checkpoint dict\nuv run python -c \"import torch; ckpt = torch.load('path/to/checkpoint.ckpt', map_location='cpu'); print('Checkpoint keys:', list(ckpt.keys()))\"\n</code></pre>"},{"location":"dl_features/evaluation/#slow-evaluation","title":"Slow Evaluation","text":"<p>Speed up evaluation:</p> <pre><code># Increase data loading workers\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval data.num_workers=8\n\n# Enable pin memory\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval data.pin_memory=true\n\n# Use compiled model\nuv run python -m {{cookiecutter.project_slug}}.scripts.eval model.torch_compile=true\n</code></pre>"},{"location":"dl_features/superpowers/","title":"Superpowers","text":""},{"location":"dl_features/superpowers/#your-superpowers-with-hydra","title":"Your Superpowers with Hydra","text":"Override any config parameter from command line <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.max_epochs=20 model.optimizer.lr=1e-4\n</code></pre>   &gt; **Note**: You can also add new parameters with `+` sign.   <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train +model.new_param=\"owo\"\n</code></pre> Train on CPU, GPU, multi-GPU and TPU <pre><code># train on CPU\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=cpu\n\n# train on 1 GPU\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=gpu\n\n# train on TPU\nuv run python -m {{cookiecutter.project_slug}}.scripts.train +trainer.tpu_cores=8\n\n# train with DDP (Distributed Data Parallel) (4 GPUs)\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=ddp trainer.devices=4\n\n# train with DDP (Distributed Data Parallel) (8 GPUs, 2 nodes)\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=ddp trainer.devices=4 trainer.num_nodes=2\n\n# simulate DDP on CPU processes\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=ddp_sim trainer.devices=2\n\n# accelerate training on mac\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=mps\n</code></pre>   &gt; **Warning**: Currently there are problems with DDP mode, read [this issue](https://github.com/ashleve/lightning-hydra-template/issues/393) to learn more.   Train with mixed precision <pre><code># train with pytorch native automatic mixed precision (AMP)\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=gpu +trainer.precision=16\n</code></pre> Train model with any logger available in PyTorch Lightning, like W&amp;B or Tensorboard <pre><code># set project and entity names in `configs/logger/wandb`\nwandb:\n  project: \"your_project_name\"\n  entity: \"your_wandb_team_name\"\n</code></pre> <pre><code># train model with Weights&amp;Biases (link to wandb dashboard should appear in the terminal)\nuv run python -m {{cookiecutter.project_slug}}.scripts.train logger=wandb\n</code></pre>   &gt; **Note**: Lightning provides convenient integrations with most popular logging frameworks. Learn more [here](#experiment-tracking).  &gt; **Note**: Using wandb requires you to [setup account](https://www.wandb.com/) first. After that just complete the config as below.  &gt; **Note**: Click [here](https://wandb.ai/hobglob/template-dashboard/) to see example wandb dashboard generated with this template.   Train model with chosen experiment config <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train experiment=example\n</code></pre>   &gt; **Note**: Experiment configs are placed in [configs/experiment/]({{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/experiment/).   Attach some callbacks to run <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train callbacks=default\n</code></pre>   &gt; **Note**: Callbacks can be used for things such as as model checkpointing, early stopping and [many more](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#built-in-callbacks).  &gt; **Note**: Callbacks configs are placed in [configs/callbacks/]({{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/callbacks/).   Use different tricks available in Pytorch Lightning <pre><code># gradient clipping may be enabled to avoid exploding gradients\nuv run python -m {{cookiecutter.project_slug}}.scripts.train +trainer.gradient_clip_val=0.5\n\n# run validation loop 4 times during a training epoch\nuv run python -m {{cookiecutter.project_slug}}.scripts.train +trainer.val_check_interval=0.25\n\n# accumulate gradients\nuv run python -m {{cookiecutter.project_slug}}.scripts.train +trainer.accumulate_grad_batches=10\n\n# terminate training after 12 hours\nuv run python -m {{cookiecutter.project_slug}}.scripts.train +trainer.max_time=\"00:12:00:00\"\n</code></pre>   &gt; **Note**: PyTorch Lightning provides about [40+ useful trainer flags](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags).   Easily debug <pre><code># runs 1 epoch in default debugging mode\n# changes logging directory to `logs/debugs/...`\n# sets level of all command line loggers to 'DEBUG'\n# enforces debug-friendly configuration\nuv run python -m {{cookiecutter.project_slug}}.scripts.train debug=default\n\n# run 1 train, val and test loop, using only 1 batch\nuv run python -m {{cookiecutter.project_slug}}.scripts.train debug=fdr\n\n# print execution time profiling\nuv run python -m {{cookiecutter.project_slug}}.scripts.train debug=profiler\n\n# try overfitting to 1 batch\nuv run python -m {{cookiecutter.project_slug}}.scripts.train debug=overfit\n\n# raise exception if there are any numerical anomalies in tensors, like NaN or +/-inf\nuv run python -m {{cookiecutter.project_slug}}.scripts.train +trainer.detect_anomaly=true\n\n# use only 20% of the data\nuv run python -m {{cookiecutter.project_slug}}.scripts.train +trainer.limit_train_batches=0.2 \\\n+trainer.limit_val_batches=0.2 +trainer.limit_test_batches=0.2\n</code></pre>   &gt; **Note**: Visit [configs/debug/]({{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/debug/) for different debugging configs.   Resume training from checkpoint <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train ckpt_path=\"/path/to/ckpt/name.ckpt\"\n</code></pre>   &gt; **Note**: Checkpoint can be either path or URL.  &gt; **Note**: Currently loading ckpt doesn't resume logger experiment, but it will be supported in future Lightning release.   Evaluate checkpoint on test dataset <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.eval ckpt_path=\"/path/to/ckpt/name.ckpt\"\n</code></pre>   &gt; **Note**: Checkpoint can be either path or URL.   Create a sweep over hyperparameters <pre><code># this will run 6 experiments one after the other,\n# each with different combination of batch_size and learning rate\nuv run python -m {{cookiecutter.project_slug}}.scripts.train -m data.batch_size=32,64,128 model.lr=0.001,0.0005\n</code></pre>   &gt; **Note**: Hydra composes configs lazily at job launch time. If you change code or configs after launching a job/sweep, the final composed configs might be impacted.   Create a sweep over hyperparameters with Optuna <pre><code># this will run hyperparameter search defined in `configs/hparams_search/mnist_optuna.yaml`\n# over chosen experiment config\nuv run python -m {{cookiecutter.project_slug}}.scripts.train -m hparams_search=mnist_optuna experiment=example\n</code></pre>   &gt; **Note**: Using [Optuna Sweeper](https://hydra.cc/docs/next/plugins/optuna_sweeper) doesn't require you to add any boilerplate to your code, everything is defined in a [single config file]({{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/hparams_search/mnist_optuna.yaml).  &gt; **Warning**: Optuna sweeps are not failure-resistant (if one job crashes then the whole sweep crashes).   Execute all experiments from folder <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train -m 'experiment=glob(*)'\n</code></pre>   &gt; **Note**: Hydra provides special syntax for controlling behavior of multiruns. Learn more [here](https://hydra.cc/docs/next/tutorials/basic/running_your_app/multi-run). The command above executes all experiments from [configs/experiment/]({{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/experiment/).   Execute run for multiple different seeds <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train -m seed=1,2,3,4,5 trainer.deterministic=True logger=csv tags=[\"benchmark\"]\n</code></pre>   &gt; **Note**: `trainer.deterministic=True` makes pytorch more deterministic but impacts the performance.   Execute sweep on a remote AWS cluster  &gt; **Note**: This should be achievable with simple config using [Ray AWS launcher for Hydra](https://hydra.cc/docs/next/plugins/ray_launcher). Example is not implemented in this template.   Use Hydra tab completion  &gt; **Note**: Hydra allows you to autocomplete config argument overrides in shell as you write them, by pressing `tab` key. Read the [docs](https://hydra.cc/docs/tutorials/basic/running_your_app/tab_completion).   Apply pre-commit hooks <pre><code>pre-commit run -a\n</code></pre>   &gt; **Note**: Apply pre-commit hooks to do things like auto-formatting code and configs, performing code analysis or removing output from jupyter notebooks. See [# Best Practices](#best-practices) for more.  Update pre-commit hook versions in `.pre-commit-config.yaml` with:   <pre><code>pre-commit autoupdate\n</code></pre> Run tests <pre><code># run all tests\npytest\n\n# run tests from specific file\npytest tests/test_train.py\n\n# run all tests except the ones marked as slow\npytest -k \"not slow\"\n</code></pre> Use tags  Each experiment should be tagged in order to easily filter them across files or in logger UI:   <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train tags=[\"mnist\",\"experiment_X\"]\n</code></pre>   &gt; **Note**: You might need to escape the bracket characters in your shell with `uv run python -m {{cookiecutter.project_slug}}.scripts.train tags=\\[\"mnist\",\"experiment_X\"\\]`.  If no tags are provided, you will be asked to input them from command line:   <pre><code>&gt;&gt;&gt; uv run python -m {{cookiecutter.project_slug}}.scripts.train tags=[]\n[2022-07-11 15:40:09,358][src.utils.utils][INFO] - Enforcing tags! &lt;cfg.extras.enforce_tags=True&gt;\n[2022-07-11 15:40:09,359][src.utils.rich_utils][WARNING] - No tags provided in config. Prompting user to input tags...\nEnter a list of comma separated tags (dev):\n</code></pre>   If no tags are provided for multirun, an error will be raised:   <pre><code>&gt;&gt;&gt; uv run python -m {{cookiecutter.project_slug}}.scripts.train -m +x=1,2,3 tags=[]\nValueError: Specify tags before launching a multirun!\n</code></pre>   &gt; **Note**: Appending lists from command line is currently not supported in hydra :("},{"location":"dl_features/training/","title":"Training","text":"<p>This guide covers how to train models using the PyTorch Lightning framework with Hydra configuration management.</p>"},{"location":"dl_features/training/#basic-training","title":"Basic Training","text":""},{"location":"dl_features/training/#quick-start","title":"Quick Start","text":"<p>To start training with default configuration:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train\n</code></pre> <p>This will use the default configuration defined in your project's config files.</p>"},{"location":"dl_features/training/#training-with-experiment-configs","title":"Training with Experiment Configs","text":"<p>It is recommended to use predefined experiment configurations for reproducible training setups and run the experiment with <code>experiment=new-defined-one</code>.</p> <p>Experiment configs allow you to overwrite parameters from main config. For example, you can use them to version control best hyperparameters for each combination of model and dataset.</p> Show example experiment config <pre><code># @package _global_\n\n# to execute this experiment run:\n# uv run python -m {{cookiecutter.project_slug}}.scripts.train experiment=example\n\ndefaults:\n  - override /data: mnist.yaml\n  - override /model: mnist.yaml\n  - override /callbacks: default.yaml\n  - override /trainer: default.yaml\n\n# all parameters below will be merged with parameters from default configurations set above\n# this allows you to overwrite only specified parameters\n\ntags: [\"mnist\", \"simple_dense_net\"]\n\nseed: 12345\n\ntrainer:\n  min_epochs: 10\n  max_epochs: 10\n  gradient_clip_val: 0.5\n\nmodel:\n  optimizer:\n    lr: 0.002\n  net:\n    lin1_size: 128\n    lin2_size: 256\n    lin3_size: 64\n\ndata:\n  batch_size: 64\n\nlogger:\n  wandb:\n    tags: ${tags}\n    group: \"mnist\"\n</code></pre>"},{"location":"dl_features/training/#training-with-custom-parameters","title":"Training with Custom Parameters","text":"<p>You can override any configuration parameter directly from the command line:</p> <pre><code># Change learning rate and batch size\nuv run python -m {{cookiecutter.project_slug}}.scripts.train model.optimizer.lr=0.001 data.batch_size=64\n\n# Train for specific number of epochs\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.max_epochs=50\n\n# Use different model architecture\nuv run python -m {{cookiecutter.project_slug}}.scripts.train model=mnist\n</code></pre>"},{"location":"dl_features/training/#hardware-configuration","title":"Hardware Configuration","text":""},{"location":"dl_features/training/#gpu-training","title":"GPU Training","text":"<p>PyTorch Lightning automatically detects and uses available GPUs:</p> <pre><code># Single GPU training (automatic)\nuv run python -m {{cookiecutter.project_slug}}.scripts.train\n\n# Specify number of GPUs\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.devices=2\n\n# Multi-GPU training with specific strategy\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.devices=4 trainer.strategy=ddp\n</code></pre>"},{"location":"dl_features/training/#cpu-training","title":"CPU Training","text":"<p>Force CPU training:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.accelerator=cpu\n</code></pre>"},{"location":"dl_features/training/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Enable automatic mixed precision for faster training:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.precision=16-mixed\n</code></pre>"},{"location":"dl_features/training/#advanced-training-options","title":"Advanced Training Options","text":""},{"location":"dl_features/training/#resume-training","title":"Resume Training","text":"<p>Resume from a checkpoint:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train ckpt_path=\"/path/to/checkpoint.ckpt\"\n</code></pre>"},{"location":"dl_features/training/#training-with-different-experiments","title":"Training with Different Experiments","text":"<p>Use predefined experiment configurations:</p> <pre><code># Use specific experiment config\nuv run python -m {{cookiecutter.project_slug}}.scripts.train experiment=mnist_experiment\n\n# Override experiment parameters\nuv run python -m {{cookiecutter.project_slug}}.scripts.train experiment=mnist_experiment model.optimizer.lr=0.01\n</code></pre>"},{"location":"dl_features/training/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"dl_features/training/#tensorboard-default","title":"TensorBoard (Default)","text":"<p>TensorBoard logs are automatically saved. View them with:</p> <pre><code>tensorboard --logdir logs/\n</code></pre>"},{"location":"dl_features/training/#weights-biases","title":"Weights &amp; Biases","text":"<p>If configured, enable W&amp;B logging:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train logger=wandb\n</code></pre>"},{"location":"dl_features/training/#multiple-loggers","title":"Multiple Loggers","text":"<p>Use multiple loggers simultaneously:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train logger=many_loggers\n</code></pre>"},{"location":"dl_features/training/#training-strategies","title":"Training Strategies","text":""},{"location":"dl_features/training/#single-machine-training","title":"Single Machine Training","text":"<pre><code># Standard single GPU\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.devices=1\n\n# Single machine, multiple GPUs\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=ddp trainer.devices=4\n</code></pre>"},{"location":"dl_features/training/#distributed-training","title":"Distributed Training","text":"<p>For multi-node training, 2 nodes, 4 gpu in each node</p> <pre><code>export MASTER_PORT=1234\nexport MASTER_ADDR=$MASTER_ADDR\nexport WORLD_SIZE=$NUM_NODES\nexport NODE_RANK=$NODE_RANK\n\n# Node 0 (master)\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.devices=4 trainer.num_nodes=$NUM_NODES\n\n# Node 1\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.devices=4 trainer.num_nodes=$NUM_NODES\n</code></pre>"},{"location":"dl_features/training/#model-checkpointing","title":"Model Checkpointing","text":""},{"location":"dl_features/training/#automatic-checkpointing","title":"Automatic Checkpointing","text":"<p>The framework automatically saves checkpoints based on validation metrics:</p> <pre><code># Save top 3 models based on validation accuracy\nuv run python -m {{cookiecutter.project_slug}}.scripts.train callbacks.model_checkpoint.save_top_k=3 \\\n                   callbacks.model_checkpoint.monitor=\"val/acc\"\n</code></pre>"},{"location":"dl_features/training/#manual-checkpointing","title":"Manual Checkpointing","text":"<p>Save checkpoints at regular intervals:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train callbacks.model_checkpoint.every_n_epochs=10\n</code></pre>"},{"location":"dl_features/training/#training-monitoring","title":"Training Monitoring","text":""},{"location":"dl_features/training/#progress-bars","title":"Progress Bars","text":"<p>Customize training progress display:</p> <pre><code># change different progress bar\nuv run python -m {{cookiecutter.project_slug}}.scripts.train callback.progress_bar=rich\n</code></pre>"},{"location":"dl_features/training/#early-stopping","title":"Early Stopping","text":"<p>Enable early stopping to prevent overfitting:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train callbacks.early_stopping.monitor=\"val/loss\" \\\n                   callbacks.early_stopping.patience=10 \\\n                   callbacks.early_stopping.mode=\"min\"\n</code></pre>"},{"location":"dl_features/training/#common-training-workflows","title":"Common Training Workflows","text":""},{"location":"dl_features/training/#development-training","title":"Development Training","text":"<p>Quick training for development and debugging:</p> <pre><code># limit training data\nuv run python -m {{cookiecutter.project_slug}}.scripts.train debug=limit\n</code></pre>"},{"location":"dl_features/training/#production-training","title":"Production Training","text":"<p>Full training with all features enabled:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train \\\n  trainer.max_epochs=100 \\\n  trainer=ddp \\\n  trainer.devices=4 \\\n  trainer.precision=16-mixed \\\n  callbacks.model_checkpoint.save_top_k=5 \\\n  callbacks.early_stopping.patience=15\n</code></pre>"},{"location":"dl_features/training/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Use Hydra's multirun feature for hyperparameter sweeps:</p> <pre><code># Grid search over learning rates\nuv run python -m {{cookiecutter.project_slug}}.scripts.train -m model.optimizer.lr=0.001,0.01,0.1\n\n# Random search with Optuna\nuv run python -m {{cookiecutter.project_slug}}.scripts.train -m hparams_search=optuna experiment=example\n</code></pre>"},{"location":"dl_features/training/#debugging-training","title":"Debugging Training","text":""},{"location":"dl_features/training/#debug-mode","title":"Debug Mode","text":"<p>Enable debug mode for detailed logging:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train debug=default\n</code></pre>"},{"location":"dl_features/training/#profiling","title":"Profiling","text":"<p>Profile your training code:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train debug=profiler trainer.profiler=simple\n\n# Advanced profiling\nuv run python -m {{cookiecutter.project_slug}}.scripts.train debug=profiler trainer.profiler=advanced\n</code></pre>"},{"location":"dl_features/training/#detect-anomalies","title":"Detect Anomalies","text":"<p>Enable anomaly detection:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train trainer.detect_anomaly=true\n</code></pre>"},{"location":"dl_features/training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"dl_features/training/#out-of-memory-oom","title":"Out of Memory (OOM)","text":"<ul> <li>Reduce batch size: <code>data.batch_size=16</code></li> <li>Use mixed precision: <code>trainer.precision=16-mixed</code></li> </ul>"},{"location":"dl_features/training/#slow-training","title":"Slow Training","text":"<ul> <li>Increase number of workers: <code>data.num_workers=8</code></li> <li>Use faster data loading: <code>data.pin_memory=true</code></li> <li>Enable compiled model: <code>model.compile=true</code></li> </ul>"},{"location":"dl_features/training/#unstable-training","title":"Unstable Training","text":"<ul> <li>Reduce learning rate: <code>model.optimizer.lr=0.0001</code></li> <li>Add gradient clipping: <code>trainer.gradient_clip_val=0.5</code></li> <li>Use learning rate scheduler: <code>model.scheduler.step_size=30</code></li> </ul>"},{"location":"dl_features/tutorials/","title":"Hydra Configuration","text":"<p>This project uses Hydra for flexible and composable configuration management, enabling organized and reproducible machine learning experiments.</p>"},{"location":"dl_features/tutorials/#how-it-works","title":"How It Works","text":"<p>All PyTorch Lightning modules are dynamically instantiated from module paths specified in config. Example model config:</p> <pre><code>_target_: src.models.mnist_model.MNISTLitModule\nlr: 0.001\nnet:\n  _target_: src.models.components.simple_dense_net.SimpleDenseNet\n  input_size: 784\n  lin1_size: 256\n  lin2_size: 256\n  lin3_size: 256\n  output_size: 10\n</code></pre> <p>Using this config we can instantiate the object with the following line:</p> <pre><code>model = hydra.utils.instantiate(config.model)\n</code></pre> <p>This allows you to easily iterate over new models! Every time you create a new one, just specify its module path and parameters in appropriate config file. </p> <p>Switch between models and datamodules with command line arguments:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train model=mnist\n</code></pre> <p>Example pipeline managing the instantiation logic: scripts/train.py.</p> <p></p>"},{"location":"dl_features/tutorials/#main-config","title":"Main Config","text":"<p>Location: [configs/train.yaml]https://github.com/foreverYoungGitHub/cookiecutter-pytorch-lightning/tree/main/{{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/configs/train.yaml)  Main project config contains default training configuration. It determines how config is composed when simply executing command <code>uv run python -m {{cookiecutter.project_slug}}.scripts.train</code>.</p> Show main project config <pre><code># order of defaults determines the order in which configs override each other\ndefaults:\n  - _self_\n  - data: mnist.yaml\n  - model: mnist.yaml\n  - callbacks: default.yaml\n  - logger: null # set logger here or use command line (e.g. `uv run python -m {{cookiecutter.project_slug}}.scripts.train logger=csv`)\n  - trainer: default.yaml\n  - paths: default.yaml\n  - extras: default.yaml\n  - hydra: default.yaml\n\n  # experiment configs allow for version control of specific hyperparameters\n  # e.g. best hyperparameters for given model and datamodule\n  - experiment: null\n\n  # config for hyperparameter optimization\n  - hparams_search: null\n\n  # optional local config for machine/user specific settings\n  # it's optional since it doesn't need to exist and is excluded from version control\n  - optional local: default.yaml\n\n  # debugging config (enable through command line, e.g. `uv run python -m {{cookiecutter.project_slug}}.scripts.train debug=default)\n  - debug: null\n\n# task name, determines output directory path\ntask_name: \"train\"\n\n# tags to help you identify your experiments\n# you can overwrite this in experiment configs\n# overwrite from command line with `uv run python -m {{cookiecutter.project_slug}}.scripts.train tags=\"[first_tag, second_tag]\"`\n# appending lists from command line is currently not supported :(\n# https://github.com/facebookresearch/hydra/issues/1547\ntags: [\"dev\"]\n\n# set False to skip model training\ntrain: True\n\n# evaluate on test set, using best model weights achieved during training\n# lightning chooses best weights based on the metric specified in checkpoint callback\ntest: True\n\n# simply provide checkpoint path to resume training\nckpt_path: null\n\n# seed for random number generators in pytorch, numpy and python.random\nseed: null\n</code></pre> <p></p>"},{"location":"dl_features/tutorials/#experiment-config","title":"Experiment Config","text":"<p>Location: [configs/experiment]https://github.com/foreverYoungGitHub/cookiecutter-pytorch-lightning/tree/main/{{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/configs/experiment) Experiment configs allow you to overwrite parameters from main config. For example, you can use them to version control best hyperparameters for each combination of model and dataset.</p> Show example experiment config <pre><code># @package _global_\n\n# to execute this experiment run:\n# uv run python -m {{cookiecutter.project_slug}}.scripts.train experiment=example\n\ndefaults:\n  - override /data: mnist.yaml\n  - override /model: mnist.yaml\n  - override /callbacks: default.yaml\n  - override /trainer: default.yaml\n\n# all parameters below will be merged with parameters from default configurations set above\n# this allows you to overwrite only specified parameters\n\ntags: [\"mnist\", \"simple_dense_net\"]\n\nseed: 12345\n\ntrainer:\n  min_epochs: 10\n  max_epochs: 10\n  gradient_clip_val: 0.5\n\nmodel:\n  optimizer:\n    lr: 0.002\n  net:\n    lin1_size: 128\n    lin2_size: 256\n    lin3_size: 64\n\ndata:\n  batch_size: 64\n\nlogger:\n  wandb:\n    tags: ${tags}\n    group: \"mnist\"\n</code></pre> <p></p>"},{"location":"dl_features/tutorials/#workflow","title":"Workflow","text":"<p>Basic workflow</p> <ol> <li>Write your PyTorch Lightning module (see models/mnist_module.py for example)</li> <li>Write your PyTorch Lightning datamodule (see data/mnist_datamodule.py for example)</li> <li>Write your experiment config, containing paths to model and datamodule</li> <li>Run training with chosen experiment config:    <code>bash    uv run python -m {{cookiecutter.project_slug}}.scripts.train experiment=experiment_name.yaml</code></li> </ol> <p>Experiment design</p> <p>Say you want to execute many runs to plot how accuracy changes in respect to batch size.</p> <ol> <li>Execute the runs with some config parameter that allows you to identify them easily, like tags:</li> </ol> <p><code>bash    uv run python -m {{cookiecutter.project_slug}}.scripts.train -m logger=csv data.batch_size=16,32,64,128 tags=[\"batch_size_exp\"]</code></p> <ol> <li>Write a script or notebook that searches over the <code>logs/</code> folder and retrieves csv logs from runs containing given tags in config. Plot the results.</li> </ol> <p></p>"},{"location":"dl_features/tutorials/#logs","title":"Logs","text":"<p>Hydra creates new output directory for every executed run.</p> <p>Default logging structure:</p> <pre><code>\u251c\u2500\u2500 logs\n\u2502   \u251c\u2500\u2500 task_name\n\u2502   \u2502   \u251c\u2500\u2500 runs                        # Logs generated by single runs\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 YYYY-MM-DD_HH-MM-SS       # Datetime of the run\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 .hydra                  # Hydra logs\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 csv                     # Csv logs\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 wandb                   # Weights&amp;Biases logs\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 checkpoints             # Training checkpoints\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ...                     # Any other thing saved during training\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 multiruns                   # Logs generated by multiruns\n\u2502   \u2502       \u251c\u2500\u2500 YYYY-MM-DD_HH-MM-SS       # Datetime of the multirun\n\u2502   \u2502       \u2502   \u251c\u2500\u25001                        # Multirun job number\n\u2502   \u2502       \u2502   \u251c\u2500\u25002\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 debugs                          # Logs generated when debugging config is attached\n\u2502       \u2514\u2500\u2500 ...\n</code></pre> <p>You can change this structure by modifying paths in hydra configuration.</p> <p></p>"},{"location":"dl_features/tutorials/#experiment-tracking","title":"Experiment Tracking","text":"<p>PyTorch Lightning supports many popular logging frameworks: Weights&amp;Biases, Neptune, Comet, MLFlow, Tensorboard.</p> <p>These tools help you keep track of hyperparameters and output metrics and allow you to compare and visualize results. To use one of them simply complete its configuration in [configs/logger]https://github.com/foreverYoungGitHub/cookiecutter-pytorch-lightning/tree/main/{{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/configs/logger) and run:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train logger=logger_name\n</code></pre> <p>You can use many of them at once (see [configs/logger/many_loggers.yaml]https://github.com/foreverYoungGitHub/cookiecutter-pytorch-lightning/tree/main/{{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/configs/logger/many_loggers.yaml) for example).</p> <p>You can also write your own logger.</p> <p>Lightning provides convenient method for logging custom metrics from inside LightningModule. Read the docs or take a look at MNIST example.</p> <p></p>"},{"location":"dl_features/tutorials/#tests","title":"Tests","text":"<p>Template comes with generic tests implemented with <code>pytest</code>.</p> <pre><code># run all tests\nmake test\n</code></pre> <p>Most of the implemented tests don't check for any specific output - they exist to simply verify that executing some commands doesn't end up in throwing exceptions. You can execute them once in a while to speed up the development.</p> <p>Currently, the tests cover cases like:</p> <ul> <li>running 1 train, val and test step</li> <li>running 1 epoch on 1% of data, saving ckpt and resuming for the second epoch</li> <li>running 2 epochs on 1% of data, with DDP simulated on CPU</li> </ul> <p>And many others. You should be able to modify them easily for your use case.</p> <p>There is also <code>@RunIf</code> decorator implemented, that allows you to run tests only if certain conditions are met, e.g. GPU is available or system is not windows. See the examples.</p> <p></p>"},{"location":"dl_features/tutorials/#hyperparameter-search","title":"Hyperparameter Search","text":"<p>You can define hyperparameter search by adding new config file to [configs/hparams_search]https://github.com/foreverYoungGitHub/cookiecutter-pytorch-lightning/tree/main/{{cookiecutter.project_name}}/{{cookiecutter.project_slug}}/configs/hparams_search).</p> Show example hyperparameter search config <pre><code># @package _global_\n\ndefaults:\n  - override /hydra/sweeper: optuna\n\n# choose metric which will be optimized by Optuna\n# make sure this is the correct name of some metric logged in lightning module!\noptimized_metric: \"val/acc_best\"\n\n# here we define Optuna hyperparameter search\n# it optimizes for value returned from function with @hydra.main decorator\nhydra:\n  sweeper:\n    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper\n\n    # 'minimize' or 'maximize' the objective\n    direction: maximize\n\n    # total number of runs that will be executed\n    n_trials: 20\n\n    # choose Optuna hyperparameter sampler\n    # docs: https://optuna.readthedocs.io/en/stable/reference/samplers.html\n    sampler:\n      _target_: optuna.samplers.TPESampler\n      seed: 1234\n      n_startup_trials: 10 # number of random sampling runs before optimization starts\n\n    # define hyperparameter search space\n    params:\n      model.optimizer.lr: interval(0.0001, 0.1)\n      data.batch_size: choice(32, 64, 128, 256)\n      model.net.lin1_size: choice(64, 128, 256)\n      model.net.lin2_size: choice(64, 128, 256)\n      model.net.lin3_size: choice(32, 64, 128, 256)\n</code></pre> <p>Next, execute it with: <code>uv run python -m {{cookiecutter.project_slug}}.scripts.train -m hparams_search=mnist_optuna</code></p> <p>Using this approach doesn't require adding any boilerplate to code, everything is defined in a single config file. The only necessary thing is to return the optimized metric value from the launch file.</p> <p>You can use different optimization frameworks integrated with Hydra, like Optuna, Ax or Nevergrad.</p> <p>The <code>optimization_results.yaml</code> will be available under <code>logs/task_name/multirun</code> folder.</p> <p>This approach doesn't support resuming interrupted search and advanced techniques like prunning - for more sophisticated search and workflows, you should probably write a dedicated optimization task (without multirun feature).</p> <p></p>"},{"location":"dl_features/tutorials/#continuous-integration","title":"Continuous Integration","text":"<p>Template comes with CI workflows implemented in Github Actions:</p> <ul> <li><code>.github/workflows/test.yaml</code>: running all tests with pytest</li> <li><code>.github/workflows/code-quality-main.yaml</code>: running pre-commits on main branch for all files</li> <li><code>.github/workflows/code-quality-pr.yaml</code>: running pre-commits on pull requests for modified files only</li> </ul> <p></p>"},{"location":"dl_features/tutorials/#distributed-training","title":"Distributed Training","text":"<p>Lightning supports multiple ways of doing distributed training. The most common one is DDP, which spawns separate process for each GPU and averages gradients between them. To learn about other approaches read the lightning docs.</p> <p>You can run DDP on mnist example with 4 GPUs like this:</p> <pre><code>uv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=ddp\n</code></pre> <p>Note: When using DDP you have to be careful how you write your models - read the docs.</p> <p></p>"},{"location":"dl_features/tutorials/#accessing-datamodule-attributes-in-model","title":"Accessing Datamodule Attributes In Model","text":"<p>The simplest way is to pass datamodule attribute directly to model on initialization:</p> <pre><code># ./src/train.py\ndatamodule = hydra.utils.instantiate(config.data)\nmodel = hydra.utils.instantiate(config.model, some_param=datamodule.some_param)\n</code></pre> <p>Note: Not a very robust solution, since it assumes all your datamodules have <code>some_param</code> attribute available.</p> <p>Similarly, you can pass a whole datamodule config as an init parameter:</p> <pre><code># ./src/train.py\nmodel = hydra.utils.instantiate(config.model, dm_conf=config.data, _recursive_=False)\n</code></pre> <p>You can also pass a datamodule config parameter to your model through variable interpolation:</p> <pre><code># ./configs/model/my_model.yaml\n_target_: src.models.my_module.MyLitModule\nlr: 0.01\nsome_param: ${data.some_param}\n</code></pre> <p>Another approach is to access datamodule in LightningModule directly through Trainer:</p> <pre><code># ./src/models/mnist_module.py\ndef on_train_start(self):\n  self.some_param = self.trainer.datamodule.some_param\n</code></pre> <p>Note: This only works after the training starts since otherwise trainer won't be yet available in LightningModule.</p> <p></p>"},{"location":"dl_features/tutorials/#configuration-composition","title":"Configuration Composition","text":""},{"location":"dl_features/tutorials/#runtime-configuration","title":"Runtime Configuration","text":"<p>Hydra allows you to override any configuration in runtime parameter directly from the command line without modifying config files. This enables flexible experimentation and parameter tuning. using dot notation</p> <pre><code># Use specific experiment configuration\nuv run python -m {{cookiecutter.project_slug}}.scripts.train experiment=example\n\n# Override default data\nuv run python -m {{cookiecutter.project_slug}}.scripts.train data=mnist\n\n# Override default model\nuv run python -m {{cookiecutter.project_slug}}.scripts.train model=mnist\n\n# Override default trainer\nuv run python -m {{cookiecutter.project_slug}}.scripts.train trainer=gpu\n\n# Override specific parameters\nuv run python -m {{cookiecutter.project_slug}}.scripts.train model.optimizer.lr=0.01 data.batch_size=128\n\n# Combine multiple overrides\nuv run python -m {{cookiecutter.project_slug}}.scripts.train experiment=example trainer.max_epochs=20 trainer.devices=2\n</code></pre>"},{"location":"features/cicd/","title":"CI/CD with Github actions","text":"<p>when <code>include_github_actions</code> is set to <code>\"y\"</code>, a <code>.github</code> directory is added with the following structure:</p> <pre><code>.github\n\u251c\u2500\u2500 workflows\n\u251c\u2500\u2500\u2500 run-checks\n\u2502    \u2514\u2500\u2500 action.yml\n\u251c\u2500\u2500\u2500 setup-python-env\n\u2502    \u2514\u2500\u2500 action.yml\n\u251c\u2500\u2500 on-merge-to-main.yml\n\u251c\u2500\u2500 on-pull-request.yml\n\u2514\u2500\u2500 on-release-main.yml\n</code></pre> <p><code>on-merge-to-main.yml</code> and <code>on-pull-request.yml</code> are identical except for their trigger conditions; the first is run whenever a new commit is made to <code>main</code> (which should only happen through merge requests, hence the name), and the latter is run whenever a pull request is opened or updated. They call the <code>action.yml</code> files to set-up the environment, run the tests, and check the code formatting.</p> <p><code>on-release-main.yml</code> does all of the former whenever a new release is made on the <code>main</code> branch. In addition, <code>on-release-main.yml</code> also publishes the project to PyPI if <code>publish_to_pypi</code> is set to <code>\"y\"</code>, and it builds and deploys the documentation if <code>mkdocs</code> is set to <code>\"y\"</code>. To learn more about these features, see Publishing to PyPI and Documentation with MkDocs</p> <p>Additionally, all workflows check for compatibility with multiple Python versions if <code>tox</code> is set to <code>\"y\"</code>.</p>"},{"location":"features/cicd/#how-to-trigger-a-release","title":"How to trigger a release?","text":"<p>To trigger a new release, navigate to your repository on GitHub, click <code>Releases</code> on the right, and then select <code>Draft a new release</code>. If you fail to find the button, you could also directly visit <code>https://github.com/&lt;username&gt;/&lt;repository-name&gt;/releases/new</code>.</p> <p>Give your release a title, and add a new tag in the form <code>*.*.*</code> where the <code>*</code>'s are alphanumeric. To finish, press <code>Publish release</code>.</p>"},{"location":"features/codecov/","title":"Test coverage with codecov","text":"<p>If <code>codecov</code> is set to <code>\"y\"</code>, <code>pytest-cov</code> is added as a development dependency, and <code>make test</code> will run the tests and output a coverage report as <code>coverage.xml</code>. If <code>include_github_actions</code> is set to <code>\"y\"</code>, coverage tests with codecov are added to the CI/CD pipeline. To enable this, sign up at codecov.io with your GitHub account. If codecov is configured to require token authentication for the upload, then follow the instructions for token generation and how to add it on your GitHub repository. Additionally, a <code>codecov.yaml</code> file is created, with the following defaults:</p> <pre><code># Badge color changes from red to green between 70% and 100%\n# PR pipeline fails if codecov falls with 1%\n\ncoverage:\n  range: 70..100\n  round: down\n  precision: 1\n  status:\n    project:\n      default:\n        target: auto\n        threshold: 1%\n\n# Ignoring Paths\n# --------------\n# which folders/files to ignore\nignore:\n  - \"foo/bar.py\"\n</code></pre> <p>If <code>codecov</code> is set to <code>\"n\"</code>, <code>pytest-cov</code> is not added to the development dependencies and the github actions won't produce a coverage report.</p>"},{"location":"features/devcontainer/","title":"Reproducible development environments with VSCode devcontainers","text":"<p>If <code>devcontainer</code> is set to <code>\"y\"</code> project uses the VSCode devcontainer specification to create a reproducible development environment. The devcontainer is defined in the <code>.devcontainer</code> directory and pre-installs all dependencies from uv required to develop, test and build the project.</p> <p>The devcontainer also installs the pre-commit hooks and configures the VSCode python extension to use the appropriate python interpretor and pytest paths.</p>"},{"location":"features/docker/","title":"Containerization with Docker or Podman","text":"<p>If <code>dockerfile</code> is set to <code>\"y\"</code>, a simple <code>Dockerfile</code> is added to the repository. The Dockerfile installs uv, sets up the environment, and runs <code>foo.py</code> when executed.</p> <p>The container image can be built with:</p> <pre><code>docker build . -t my-container-image\n</code></pre> <p>or, if using Podman:</p> <pre><code>podman build . -t my-container-image\n</code></pre> <p>It can then be run in the background with:</p> <pre><code>docker run -d my-container-image\n</code></pre> <p>or, if using Podman:</p> <pre><code>podman run -d my-container-image\n</code></pre> <p>Alternatively, run it in interactive mode with:</p> <pre><code>docker run --rm -it --entrypoint bash my-container-image\n</code></pre> <p>or, if using Podman:</p> <pre><code>podman run --rm -it --entrypoint bash my-container-image\n</code></pre>"},{"location":"features/linting/","title":"Linting and code quality","text":"<p>Code can be linted and quality-checked with the command</p> <pre><code>make check\n</code></pre> <p>Note that this requires the pre-commit hooks to be installed.</p> <p>This command will run the following tools:</p>"},{"location":"features/linting/#ruff","title":"ruff","text":"<p>ruff is used to lint and format the code, and it is configured through <code>pyproject.toml</code>:</p> <pre><code>[tool.ruff]\ntarget-version = \"py39\"\nline-length = 120\nfix = true\nselect = [\n    # flake8-2020\n    \"YTT\",\n    # flake8-bandit\n    \"S\",\n    # flake8-bugbear\n    \"B\",\n    # flake8-builtins\n    \"A\",\n    # flake8-comprehensions\n    \"C4\",\n    # flake8-debugger\n    \"T10\",\n    # flake8-simplify\n    \"SIM\",\n    # isort\n    \"I\",\n    # mccabe\n    \"C90\",\n    # pycodestyle\n    \"E\", \"W\",\n    # pyflakes\n    \"F\",\n    # pygrep-hooks\n    \"PGH\",\n    # pyupgrade\n    \"UP\",\n    # ruff\n    \"RUF\",\n    # tryceratops\n    \"TRY\",\n]\nignore = [\n    # LineTooLong\n    \"E501\",\n    # DoNotAssignLambda\n    \"E731\",\n]\n\n[tool.ruff.format]\npreview = true\n\n[tool.ruff.per-file-ignores]\n\"tests/*\" = [\"S101\"]\n</code></pre>"},{"location":"features/linting/#mypy","title":"mypy","text":"<p>mypy is used for static type checking, and it's configuration and can be edited in <code>pyproject.toml</code>.</p> <pre><code>[tool.mypy]\ndisallow_untyped_defs = true\ndisallow_any_unimported = true\nno_implicit_optional = true\ncheck_untyped_defs = true\nwarn_return_any = true\nwarn_unused_ignores = true\nshow_error_codes = true\nexclude = [\n    '\\.venv',\n    '{{cookiecutter.project_name}}',\n    'tests'\n]\n</code></pre>"},{"location":"features/linting/#deptry","title":"deptry","text":"<p>deptry is used to check the code for dependency issues, and it can be configured by adding a <code>[tool.deptry]</code> section in <code>pyproject.toml</code>. For more information, see this section documentation of deptry.</p>"},{"location":"features/linting/#github-actions","title":"Github Actions","text":"<p>If <code>include_github_actions</code> is set to <code>\"y\"</code>, code formatting is checked for every merge request, every merge to main, and every release.</p>"},{"location":"features/makefile/","title":"Makefile","text":"<p>The generated repository will have a <code>Makefile</code> available. A list of all available commands that are available can be obtained by running <code>make help</code> in the terminal. Initially, if all features are selected, the following commands are available:</p> <pre><code>install              Install the uv environment and install the pre-commit hooks\ncheck                Lint and check code by running ruff, mypy and deptry.\ntest                 Test the code with pytest\nbuild                Build wheel file using uv\nclean-build          clean build artifacts\npublish              publish a release to pypi.\nbuild-and-publish    Build and publish.\ndocs-test            Test if documentation can be built without warnings or errors\ndocs                 Build and serve the documentation\n</code></pre>"},{"location":"features/mkdocs/","title":"Documentation with MkDocs","text":"<p>If <code>mkdocs</code> is set to <code>\"y\"</code>, documentation of your project is automatically added using MkDocs. Next to that, if <code>\"include_github_actions\"</code> is set to <code>\"y\"</code>, the documentation is automatically deployed to your <code>gh-pages</code> branch, and made available at <code>https://&lt;github_handle&gt;.github.io/&lt;project_name&gt;/</code>.</p> <p>To view the documentation locally, simply run</p> <pre><code>make docs\n</code></pre> <p>This command will generate and build your documentation, and start the server locally so you can access it at http://localhost:8000.</p>"},{"location":"features/mkdocs/#enabling-the-documentation-on-github","title":"Enabling the documentation on GitHub","text":"<p>To enable your documentation on GitHub, first navigate to <code>Settings &gt; Actions &gt; General</code> in your repository, and under <code>Workflow permissions</code> select <code>Read and write permissions</code></p> <p>Then, create a new release for your project.</p> <p>Then, in your repository, navigate to <code>Settings &gt; Code and Automation &gt; Pages</code>. If you succesfully created a new release, you should see a notification saying <code>Your site is ready to be published at https://&lt;author_github_handle&gt;.github.io/&lt;project_name&gt;/</code>.</p> <p>To finalize deploying your documentation, under <code>Source</code>, select the branch <code>gh-pages</code>. Your documentation should then be live within a few minutes.</p>"},{"location":"features/mkdocs/#documenting-docstrings","title":"Documenting docstrings","text":"<p>The generated project also converts all your docstrings into legible documentation. By default, the project is configured to work with google style docstrings.</p> <p>An example of a Google style docstring:</p> <pre><code>def function_with_pep484_type_annotations(param1: int, param2: str) -&gt; bool:\n\"\"\"Example function with PEP 484 type annotations.\n\nArgs:\n    param1: The first parameter.\n    param2: The second parameter.\n\nReturns:\n    The return value. True for success, False otherwise.\n</code></pre> <p>For more examples, see here.</p>"},{"location":"features/publishing/","title":"Publishing to PyPI","text":""},{"location":"features/publishing/#releasing-from-github","title":"Releasing from Github","text":"<p>When <code>publish_to_pypi</code> is set to <code>\"y\"</code>, the <code>on-release-main.yml</code> workflow publishes the code to PyPI whenever a new release is made.</p> <p>Before you can succesfully publish your project from the release workflow, you need to add some secrets to your github repository so they can be used as environment variables.</p>"},{"location":"features/publishing/#set-up-for-pypi","title":"Set-up for PyPI","text":"<p>In order to publish to PyPI, the secret <code>PYPI_TOKEN</code> should be set in your repository. In your Github repository, navigate to <code>Settings &gt; Secrets &gt; Actions</code> and press <code>New repository secret</code>. As the name of the secret, set <code>PYPI_TOKEN</code>. Then, in a new tab go to your PyPI Account settings and select <code>Add API token</code>. Copy and paste the token in the <code>Value</code> field for the Github secret in your first tab, and you're all set!</p>"},{"location":"features/publishing/#publishing-from-your-local-machine","title":"Publishing from your local machine","text":"<p>It is also possible to release locally, although it is not recommended. To do so, run:</p> <pre><code>make build-and-publish\n</code></pre>"},{"location":"features/pytest/","title":"Unittesting with Pytest","text":"<p>pytest is automatically added to the environment. There will be a template unittest in the <code>tests</code> directory upon creation of the project, which can be run with</p> <pre><code>make test\n</code></pre> <p>If <code>include_github_actions</code> is set to <code>\"y\"</code>, the tests are automatically run for every merge request, every merge to main, and every release.</p>"},{"location":"features/tox/","title":"Compatibility testing with Tox","text":"<p>If <code>tox</code> is set to <code>\"y\"</code> project uses tox-uv to test compatibility with multiple Python versions. You can run <code>tox</code> locally:</p> <pre><code>uv run tox\n</code></pre> <p>By default, the project is tested with Python <code>3.8</code>, <code>3.9</code>, and <code>3.10</code>, <code>3.11</code> and <code>3.12</code>.</p> <p>Testing for compatibility with different Python versions is also done automatically in the CI/CD pipeline on every pull request, merge to main, and on each release.</p> <p>If you want to test for compatbility with more Python versions you can simply add them to <code>tox.ini</code> and to the separate workflows in <code>.github</code>.</p>"},{"location":"features/uv/","title":"Dependency management with uv","text":"<p>The generated repository will uses uv for its dependency management. When you have created your repository using this cookiecutter template, a uv environment is pre-configured in <code>pyproject.toml</code>. All you need to do is add your project-specific dependencies with</p> <pre><code>uv add &lt;package&gt;\n</code></pre> <p>and then install the environment with</p> <pre><code>uv sync\n</code></pre> <p>You can then run commands within your virtual environment, for example:</p> <pre><code>uv run python -m pytest\n</code></pre>"}]}